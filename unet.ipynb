{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data\n",
      "------------------------------\n",
      "load train images...\n",
      "------------------------------\n",
      "------------------------------\n",
      "load test images...\n",
      "------------------------------\n",
      "loading data done\n",
      "conv1 shape: (?, 512, 512, 64)\n",
      "conv1 shape: (?, 512, 512, 64)\n",
      "pool1 shape: (?, 256, 256, 64)\n",
      "conv2 shape: (?, 256, 256, 128)\n",
      "conv2 shape: (?, 256, 256, 128)\n",
      "pool2 shape: (?, 128, 128, 128)\n",
      "conv3 shape: (?, 128, 128, 256)\n",
      "conv3 shape: (?, 128, 128, 256)\n",
      "pool3 shape: (?, 64, 64, 256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda_64anzhuang\\lib\\site-packages\\ipykernel_launcher.py:226: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n",
      "D:\\anaconda_64anzhuang\\lib\\site-packages\\ipykernel_launcher.py:246: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got unet\n",
      "Fitting model...\n",
      "Train on 1 samples, validate on 1 samples\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 40s 40s/step - loss: 0.6957 - acc: 0.6647 - val_loss: 0.6932 - val_acc: 0.9646\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.69569, saving model to my_unet.hdf5\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 35s 35s/step - loss: 0.6931 - acc: 0.9199 - val_loss: 0.6931 - val_acc: 0.9705\n",
      "\n",
      "Epoch 00002: loss improved from 0.69569 to 0.69314, saving model to my_unet.hdf5\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 35s 35s/step - loss: 0.6931 - acc: 0.9230 - val_loss: 0.6930 - val_acc: 0.9720\n",
      "\n",
      "Epoch 00003: loss improved from 0.69314 to 0.69307, saving model to my_unet.hdf5\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 31s 31s/step - loss: 0.6930 - acc: 0.9232 - val_loss: 0.6930 - val_acc: 0.9725\n",
      "\n",
      "Epoch 00004: loss improved from 0.69307 to 0.69304, saving model to my_unet.hdf5\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 31s 31s/step - loss: 0.6930 - acc: 0.9232 - val_loss: 0.6929 - val_acc: 0.9725\n",
      "\n",
      "Epoch 00005: loss improved from 0.69304 to 0.69300, saving model to my_unet.hdf5\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 33s 33s/step - loss: 0.6930 - acc: 0.9232 - val_loss: 0.6929 - val_acc: 0.9725\n",
      "\n",
      "Epoch 00006: loss improved from 0.69300 to 0.69296, saving model to my_unet.hdf5\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 32s 32s/step - loss: 0.6929 - acc: 0.9232 - val_loss: 0.6928 - val_acc: 0.9725\n",
      "\n",
      "Epoch 00007: loss improved from 0.69296 to 0.69291, saving model to my_unet.hdf5\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 34s 34s/step - loss: 0.6929 - acc: 0.9232 - val_loss: 0.6928 - val_acc: 0.9725\n",
      "\n",
      "Epoch 00008: loss improved from 0.69291 to 0.69288, saving model to my_unet.hdf5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import numpy as np\n",
    "from keras.models import *\n",
    "from keras.layers import Input,Conv2D, MaxPooling2D, UpSampling2D, Concatenate\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "# from data import *\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import keras\n",
    "from keras.layers.core import  Dropout\n",
    "from keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "\n",
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "        self.accuracy = {'batch':[], 'epoch':[]}\n",
    "        self.val_loss = {'batch':[], 'epoch':[]}\n",
    "        self.val_acc = {'batch':[], 'epoch':[]}\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('acc'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('acc'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
    "\n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        plt.figure()\n",
    "        # acc\n",
    "        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
    "        with open(\"accuracy.txt\", \"w\") as f:\n",
    "            f.writelines(str(iters))\n",
    "            f.writelines(str(self.accuracy[loss_type]))\n",
    "        # loss\n",
    "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        with open(\"losses.txt\", \"w\") as f:\n",
    "            f.writelines(str(self.losses[loss_type]))\n",
    "        if loss_type == 'epoch':\n",
    "            # val_acc\n",
    "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "            with open(\"val_acc.txt\", \"w\") as f:\n",
    "                f.writelines(str(self.val_acc[loss_type]))\n",
    "            # val_loss\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "            with open(\"val_loss.txt\", \"w\") as f:\n",
    "                f.writelines(str(self.val_loss[loss_type]))\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc-loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.savefig(\"dpi=600\")\n",
    "        plt.show()\n",
    "\n",
    "class dataProcess(object):\n",
    "    def __init__(self, out_rows, out_cols, data_path=\"./orig_data/image\", label_path=\"./orig_data/mask\",\n",
    "                 test_path=\"./orig_data/test\", npy_path=\"./npy_data\", img_type=\"png\"):\n",
    "        # 数据处理类，初始化\n",
    "        self.out_rows = out_rows\n",
    "        self.out_cols = out_cols\n",
    "        self.data_path = data_path\n",
    "        self.label_path = label_path\n",
    "        self.img_type = img_type\n",
    "        self.test_path = test_path\n",
    "        self.npy_path = npy_path\n",
    "\n",
    "    # 创建训练数据\n",
    "    def create_train_data(self):\n",
    "        i = 0\n",
    "        print('-' * 30)\n",
    "        print('Creating training images...')\n",
    "        print('-' * 30)\n",
    "        imgs = glob.glob(self.data_path + \"/*.\" + self.img_type)\n",
    "        img_len=len(imgs)\n",
    "        print(len(imgs))\n",
    "\n",
    "\t\t# 此处有改动，1变为3\n",
    "        imgdatas = np.ndarray((len(imgs), self.out_rows, self.out_cols, 3), dtype=np.uint8)\n",
    "        imglabels = np.ndarray((len(imgs), self.out_rows, self.out_cols, 1), dtype=np.uint8)\n",
    "        for imgname in imgs:\n",
    "            midname = imgname[imgname.rindex(\"\\\\\") + 1:]\n",
    "            img = load_img(self.data_path + \"/\" + midname, color_mode='rgb')\n",
    "            label = load_img(self.label_path + \"/\" + midname, grayscale=True)\n",
    "            img = img_to_array(img)\n",
    "            label = img_to_array(label)\n",
    "            # img = cv2.imread(self.data_path + \"/\" + midname,cv2.IMREAD_GRAYSCALE)\n",
    "            # label = cv2.imread(self.label_path + \"/\" + midname,cv2.IMREAD_GRAYSCALE)\n",
    "            # img = np.array([img])\n",
    "            # label = np.array([label])\n",
    "            imgdatas[i] = img\n",
    "            imglabels[i] = label\n",
    "            if i % 100 == 0:\n",
    "                print('Done: {0}/{1} images'.format(i, len(imgs)))\n",
    "            i += 1\n",
    "        print('loading done')\n",
    "        np.save(self.npy_path + '/imgs_train.npy', imgdatas)\n",
    "        np.save(self.npy_path + '/imgs_mask_train.npy', imglabels)\n",
    "        print('Saving to .npy files done.')\n",
    "\n",
    "    # 创建测试数据\n",
    "    def create_test_data(self):\n",
    "        i = 0\n",
    "        print('-' * 30)\n",
    "        print('Creating test images...')\n",
    "        print('-' * 30)\n",
    "        imgs = glob.glob(self.test_path + \"/*.\" + self.img_type)\n",
    "        print(len(imgs))\n",
    "        imgdatas = np.ndarray((len(imgs), self.out_rows, self.out_cols, 3), dtype=np.uint8)\n",
    "        for imgname in imgs:\n",
    "            midname = imgname[imgname.rindex(\"\\\\\") + 1:]\n",
    "            img = load_img(self.test_path + \"/\" + midname, color_mode='rgb')\n",
    "            img = img_to_array(img)\n",
    "            # img = cv2.imread(self.test_path + \"/\" + midname,cv2.IMREAD_GRAYSCALE)\n",
    "            # img = np.array([img])\n",
    "            imgdatas[i] = img\n",
    "            i += 1\n",
    "        print('loading done')\n",
    "        np.save(self.npy_path + '/imgs_test.npy', imgdatas)\n",
    "        print('Saving to imgs_test.npy files done.')\n",
    "\n",
    "    # 加载训练图片与mask\n",
    "    def load_train_data(self):\n",
    "        print('-' * 30)\n",
    "        print('load train images...')\n",
    "        print('-' * 30)\n",
    "        imgs_train = np.load(self.npy_path + \"/imgs_train.npy\")\n",
    "        imgs_mask_train = np.load(self.npy_path + \"/imgs_mask_train.npy\")\n",
    "        imgs_train = imgs_train.astype('float32')\n",
    "        imgs_mask_train = imgs_mask_train.astype('float32')\n",
    "        imgs_train /= 255\n",
    "        # mean = imgs_train.mean(axis=0)\n",
    "        # imgs_train -= mean\n",
    "        imgs_mask_train /= 255\n",
    "        # 做一个阈值处理，输出的概率值大于0.5的就认为是对象，否则认为是背景\n",
    "        imgs_mask_train[imgs_mask_train > 0.5] = 1\n",
    "        imgs_mask_train[imgs_mask_train <= 0.5] = 0\n",
    "        return imgs_train, imgs_mask_train\n",
    "\n",
    "    # 加载测试图片\n",
    "    def load_test_data(self):\n",
    "        print('-' * 30)\n",
    "        print('load test images...')\n",
    "        print('-' * 30)\n",
    "        imgs_test = np.load(self.npy_path + \"/imgs_test.npy\")\n",
    "        imgs_test = imgs_test.astype('float32')\n",
    "        imgs_test /= 255\n",
    "        # mean = imgs_test.mean(axis=0)\n",
    "        # imgs_test -= mean\n",
    "        return imgs_test\n",
    "class myUnet(object):\n",
    "\tdef __init__(self, img_rows = 512, img_cols = 512):\n",
    "\t\tself.img_rows = img_rows\n",
    "\t\tself.img_cols = img_cols\n",
    "# 参数初始化定义\n",
    "\tdef load_data(self):\n",
    "\t\tmydata = dataProcess(self.img_rows, self.img_cols)\n",
    "\t\timgs_train, imgs_mask_train = mydata.load_train_data()\n",
    "\t\timgs_test = mydata.load_test_data()\n",
    "\t\treturn imgs_train, imgs_mask_train, imgs_test\n",
    "# 载入数据\n",
    "\tdef get_unet(self):\n",
    "\t\tinputs = Input((self.img_rows, self.img_cols,3))\n",
    "\t\t# 网络结构定义\n",
    "\n",
    "\t\tconv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
    "\t\tprint (\"conv1 shape:\",conv1.shape)\n",
    "\t\tconv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
    "\t\tprint (\"conv1 shape:\",conv1.shape)\n",
    "\t\tpool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\t\tprint (\"pool1 shape:\",pool1.shape)\n",
    "\n",
    "\t\tconv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
    "\t\tprint (\"conv2 shape:\",conv2.shape)\n",
    "\t\tconv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
    "\t\tprint (\"conv2 shape:\",conv2.shape)\n",
    "\t\tpool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\t\tprint (\"pool2 shape:\",pool2.shape)\n",
    "\n",
    "\t\tconv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
    "\t\tprint (\"conv3 shape:\",conv3.shape)\n",
    "\t\tconv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
    "\t\tprint (\"conv3 shape:\",conv3.shape)\n",
    "\t\tpool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\t\tprint (\"pool3 shape:\",pool3.shape)\n",
    "\n",
    "\t\tconv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
    "\t\tconv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
    "\t\tdrop4 = Dropout(0.5)(conv4)\n",
    "\t\tpool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
    "\n",
    "\t\tconv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
    "\t\tconv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
    "\t\tdrop5 = Dropout(0.5)(conv5)\n",
    "\n",
    "\t\tup6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
    "\t\tmerge6 = Concatenate(axis=3)([drop4,up6])\n",
    "\t\tconv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
    "\t\tconv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
    "\n",
    "\t\tup7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
    "\t\tmerge7 = Concatenate(axis=3)([conv3, up7])\n",
    "\t\tconv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
    "\t\tconv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
    "\n",
    "\t\tup8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
    "\t\tmerge8 = Concatenate(axis=3)([conv2, up8])\n",
    "\t\tconv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
    "\t\tconv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
    "\n",
    "\t\tup9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
    "\t\tmerge9 = Concatenate(axis=3)([conv1, up9])\n",
    "\t\tconv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
    "\t\tconv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "\t\tconv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
    "\t\tconv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
    "\n",
    "\t\tmodel = Model(input = inputs, output = conv10)\n",
    "\t\tmodel.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\t\treturn model\n",
    "\n",
    "# 如果需要修改输入的格式，那么可以从以下开始修改，上面的结构部分不需要修改\n",
    "\tdef train(self):\n",
    "\t\tprint(\"loading data\")\n",
    "\t\t# K.set_image_data_format('channels_first')\n",
    "\t\timgs_train, imgs_mask_train, imgs_test = self.load_data()\n",
    "\t\tprint(\"loading data done\")\n",
    "\t\t#\n",
    "\t\tmodel = self.get_unet()\n",
    "\t\tprint(\"got unet\")\n",
    "\t\tmodel_checkpoint = ModelCheckpoint('my_unet.hdf5', monitor='loss',verbose=1, save_best_only=True)\n",
    "\t\tprint('Fitting model...')\n",
    "\t\thistory = LossHistory()\n",
    "\t\tmodel.fit(imgs_train, imgs_mask_train, batch_size=4, \n",
    "                  nb_epoch=10, verbose=1,validation_split=0.2,\n",
    "\t\t\t\t  shuffle=True, callbacks=[model_checkpoint,history,\n",
    "\t\t\t\tTensorBoard(log_dir=r\"./mytensotboard\",\n",
    "                            update_freq=\"batch\")])\n",
    "\t\thistory.loss_plot('epoch')\n",
    "\n",
    "# \t\tprint('predict test data')\n",
    "# \t\timgs_mask_test = model.predict(imgs_test, batch_size=1, verbose=1)\n",
    "# \t\tnp.save('./results/imgs_mask_test.npy', imgs_mask_test)\n",
    "\n",
    "\tdef save_img(self):\n",
    "\t\tprint(\"array to image\")\n",
    "\t\timgs = np.load('../results/imgs_mask_test.npy')\n",
    "\t\tfor i in range(imgs.shape[0]):\n",
    "\t\t\timg = imgs[i]\n",
    "\t\t\timg = array_to_img(img)\n",
    "\t\t\timg.save(\"./results/%d.jpg\"%(i))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmyunet = myUnet()\n",
    "\tmyunet.train()\n",
    "\t# myunet.save_img()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
